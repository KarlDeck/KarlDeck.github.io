<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 / CS280A – Project 2: Fun with Filters & Frequencies</title>
  <style>
    body { margin: 0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Inter, Helvetica, Arial; color: #0f172a; background: #ffffff; }
    header { border-bottom: 1px solid #e2e8f0; padding: 16px 20px; }
    main { max-width: 960px; margin: 0 auto; padding: 24px 20px 80px; }
    h1, h2, h3 { margin: 0 0 12px; }
    h2 { margin-top: 28px; padding-top: 8px; border-top: 1px solid #e2e8f0; }
    p { margin: 8px 0 14px; }
    .grid { display: grid; gap: 12px; grid-template-columns: repeat(12, 1fr); }
    .g-4 { grid-column: span 4; } .g-6 { grid-column: span 6; } .g-12 { grid-column: span 12; }
    @media (max-width: 860px){ .g-4,.g-6,.g-12{ grid-column: span 12; } }
    figure { margin: 0; border: 1px solid #e2e8f0; border-radius: 10px; overflow: hidden; background: #f8fafc; }
    figure img { display:block; width:100%; height:auto; }
    figcaption { padding: 8px 10px; font-size: 13px; color: #475569; background: #f1f5f9; }
    pre { background: #0f172a; color: #e2e8f0; padding: 14px; border-radius: 10px; overflow: auto; font-size: 13px; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
    .todo { color: #b91c1c; font-weight: 600; }
  </style>
</head>
<body>
  <header>
    <h1>CS180 / CS280A — Project 2: Fun with Filters & Frequencies</h1>
    <p>University of California, Berkeley — Fall 2025 · Due: Sep 26, 2025, 11:59pm PT</p>
  </header>

  <main>
    <!-- ========================= Part 1 ========================= -->
    <section id="part1">
      <h2>Part 1 — Filters & Edges</h2>

      <h3 id="p11">1.1 Convolutions from Scratch (NumPy only)</h3>
      <p><strong>Runtime:</strong> Compared to the selfimplemented code the library uses a C implementation of the convolution, which is highly optimited</p>
      <div class="grid">
        <figure class="g-6">
          <img src="part1/box9x9.png" alt="9x9 box kernel visualization" />
          <figcaption>9×9 box kernel (visualization)</figcaption>
        </figure>
        <figure class="g-6">
          <img src="part1/self_box_blur.png" alt="Self-portrait convolved with 9x9 box filter" />
          <figcaption>Grayscale self-portrait blurred by 9×9 box filter</figcaption>
        </figure>
                <figure class="g-6">
          <img src="part1/me_Dx.png" alt="Self-portrait D_x" />
          <figcaption>Self-portrait D_x</figcaption>
        </figure>
                <figure class="g-6">
          <img src="part1/me_Dy.png" alt="Self-portrait D_y" />
          <figcaption>Self-portrait D_y</figcaption>
        </figure>
      </div>
      <p><strong>Code snippets (include exactly what you used):</strong></p>

<pre><code>def conv4loops(image, kernel):

    image_height, image_width = image.shape
    kernel_height, kernel_width = kernel.shape

    pad_height = kernel_height // 2
    pad_width = kernel_width // 2

    padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width)), mode='constant', constant_values=0)

    output = np.zeros_like(image)

    for i in range(image_height):
        for j in range(image_width):
            for m in range(kernel_height):
                for n in range(kernel_width):
                    output[i, j] += padded_image[i + m, j + n] * kernel[m, n]

    return output
</code></pre>

<pre><code>def conv2loops(image, kernel):

    image_height, image_width = image.shape
    kernel_height, kernel_width = kernel.shape

    pad_height = kernel_height // 2
    pad_width = kernel_width // 2

    padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width)), mode='constant', constant_values=0)

    output = np.zeros_like(image)

    for i in range(image_height):
        for j in range(image_width):
            region = padded_image[i:i + kernel_height, j:j + kernel_width]
            output[i, j] = np.sum(region * kernel)

    return output
</code></pre>

<pre><code>
image_conv2 = conv2loops(image, box_filter)
print("Convolution with 2 loops:")
show_image(image_conv2)

image = load_image('me Small.jpeg')
# D_x
image_conv = conv2loops(image, D_x)
print("D_x:")
show_image(image_conv)
# D_y
image_conv = conv2loops(image, D_y)
print("D_y:")
show_image(image_conv)
</code></pre>

      <h3 id="p12">1.2 Finite Difference Operator</h3>
      <div class="grid">
        <figure class="g-4"><img src="part1/cameraman.png" alt="Cameraman" /><figcaption>Input (Cameraman)</figcaption></figure>
        <figure class="g-4"><img src="part1/cameraman_dx.png" alt="dx" /><figcaption>∂I/∂x via D<sub>x</sub></figcaption></figure>
        <figure class="g-4"><img src="part1/cameraman_dy.png" alt="dy" /><figcaption>∂I/∂y via D<sub>y</sub></figcaption></figure>
        <figure class="g-6"><img src="part1/cameraman_gradmag.png" alt="grad mag" /><figcaption>Gradient magnitude</figcaption></figure>
        <figure class="g-6"><img src="part1/cameraman_edges.png" alt="edges" /><figcaption>Binarized edges (threshold chosen qualitatively)</figcaption></figure>
      </div>

      <h3 id="p13">1.3 Derivative of Gaussian (DoG)</h3>
      <div class="grid">
        <figure class="g-6"><img src="part1/gaussian_kernel.png" alt="gaussian kernel" /><figcaption>Gaussian kernel (σ shown)</figcaption></figure>
        <figure class="g-6"><img src="part1/blurred_photographer.png" alt="gaussian kernel" /><figcaption>Blurred Photographer with Gaussian</figcaption></figure>
        <figure class="g-6"><img src="part1/blurred_photographer+edge.png" alt="gaussian kernel" /><figcaption>Blurred Photographer with Gaussian - one can see that there is less noise in the edges</figcaption></figure>

        <figure class="g-6"><img src="part1/dog_kernel_x_small.png" alt="DoG filters" /><figcaption>DoG filter G*D<sub>x small</sub></figcaption></figure>
        <figure class="g-6"><img src="part1/dog_kernel_y_small.png" alt="DoG filters" /><figcaption>DoG filter G*D<sub>y small</sub></figcaption></figure>
        <figure class="g-6"><img src="part1/dog_kernel_x_big.png" alt="DoG filters" /><figcaption>DoG filter G*D<sub>x big</sub></figcaption></figure>
        <figure class="g-6"><img src="part1/dog_kernel_y_big.png" alt="DoG filters" /><figcaption>DoG filter G*D<sub>y big</sub></figcaption></figure>
        <figure class="g-6"><img src="part1/cameraman_dog_dx.png" alt="DoG dx" /><figcaption>DoG response (x)</figcaption></figure>
        <figure class="g-6"><img src="part1/cameraman_dog_dy.png" alt="DoG dy" /><figcaption>DoG response (y)</figcaption></figure>
      </div>
      <p><strong>Differences:
- Less of the grass is visible
- The edges are painted more softly and thicker, without this much jitter. It seems more like an coninouse edge

</code></pre>
      <p><em>(Bells & Whistles)</em> Gradient orientations in HSV:</p>
      <figure class="g-12"><img src="part1/cameraman_orientation_hsv.png" alt="HSV orientation" /><figcaption>Hue = orientation; value = magnitude</figcaption></figure>
      <pre><code>## Gradient Orientation image
from scipy.signal import convolve2d

def gradient_orientation(image):
    delta_x = convolve2d(image, D_x, mode='same', boundary='fill', fillvalue=0)
    delta_y = convolve2d(image, D_y, mode='same', boundary='fill', fillvalue=0)
    return np.arctan(2*delta_x, 2*delta_y)

    from cv2 import getGaussianKernel, imshow
image = load_image('cameraman.png')

image_conv = gradient_orientation(image_blurr)
plt.imshow(image, cmap='hsv')
plt.axis('off')
plt.show()
</code></pre>
    </section>

    <!-- ========================= Part 2 ========================= -->
    <section id="part2">
      <h2>Part 2 — Applications</h2>

      <h3 id="p21">2.1 Image "Sharpening" (Unsharp Mask)</h3>
      <div class="grid">
        <figure class="g-4"><img src="part2/taj_original.png" alt="taj original" /><figcaption>Original (Taj)</figcaption></figure>
        <figure class="g-6"><img src="part2/taj_sharp_a.png" alt="taj sharpened a" /><figcaption>Sharpened (α = <span class="todo">0.3</span>)</figcaption></figure>
      </div>
      <p><strong>Blur → Sharpen sanity check:</strong></p>
      <div class="grid">
        <figure class="g-6"><img src="part2/sharpen_examples.png" alt="sharpen examples" /><figcaption>Sharpen examples</figcaption></figure>
        <figure class="g-6"><img src="part2/sharp_blur.png" alt="blur then sharpen" /><figcaption>Blur then sharpen (compare to source)</figcaption></figure>
      </div>

      <h3 id="p22">2.2 Hybrid Images</h3>
      <div class="grid">
        <figure class="g-6"><img src="part2/derek.png" alt="Derek" /><figcaption>Derek</figcaption></figure>
        <figure class="g-6"><img src="part2/nutmeg.png" alt="Nutmeg" /><figcaption>Nutmeg</figcaption></figure>
        <figure class="g-12"><img src="part2/hybrid_d+n.png" alt="hybrid" /><figcaption>Hybrid result (Derek ⊕ Nutmeg)</figcaption></figure>
      </div>
      <p><strong>Test:</strong></p>
      <div class="grid">
        <figure class="g-4"><img src="part2/fft_derek.png" alt="fft derek" /><figcaption>Test sigmas</figcaption></figure>
      </div>
      <p><strong>More Examples + Bells & Whistles:</strong></p>
      <div class="grid">
        <figure class="g-4"><img src="part2/color_derek.png" alt="fft derek" /><figcaption>Derek and Nutmeg in colour</figcaption></figure>
        <figure class="g-4"><img src="part2/me+julius.png" alt="fft derek" /><figcaption>Me and a friend of mine mixed in Doe library</figcaption></figure>
        <figure class="g-4"><img src="part2/me+ich.png" alt="fft derek" /><figcaption>Me and myself mixed in two very different environments and clothing. One in the library and one in Joshua Tree</figcaption></figure>
      </div>
      I Found that glasses make it much harder to maintain the illusion

      <h3 id="p23">2.3 Gaussian & Laplacian Stacks</h3>
      <div class="grid">
        <figure class="g-12"><img src="part2/oraple_levels.png" alt="stacks montage" /><figcaption>Gaussian & Laplacian stacks for apple/orange (multi-level montage)</figcaption></figure>
      </div>
      <p><strong>Szelski Fig. 3.42 (a–l) recreation:</strong></p>


      <h3 id="p24">2.4 Multiresolution Blending (Oraple)</h3>
      <div class="grid">

        <figure class="g-12"><img src="part2/orapple.png" alt="oraple" /><figcaption>Final blend (mask-based, stack-aware)</figcaption></figure>
      </div>
      <p><strong>Irregular masks + Laplacian illustration:</strong></p>
      <div class="grid">
        <figure class="g-6"><img src="part2/up-down.png" alt="custom blend 1" /><figcaption>Up-Down Blend</figcaption></figure>
        <figure class="g-6"><img src="part2/handchair.png" alt="custom blend 2" /><figcaption>Custom blend #2</figcaption></figure>
        <figure class="g-12"><img src="part2/laplacian_illustration.png" alt="laplacian levels" /><figcaption>Laplacian stack levels + masked inputs</figcaption></figure>
        <figure class="g-6"><img src="part2/warmix.png" alt="custom blend 3" /><figcaption>Custom blend #3</figcaption></figure>
        <figure class="g-6"><img src="part2/zebraeye.png" alt="custom blend 3" /><figcaption>Custom blend #3</figcaption></figure>   
      </div>

    </section>

    <section id="learning">
      <h2>Most Important Thing I Learned</h2>
      <p>The filterin with high and low pass was a good learning. I liked the gaussian and laplacian stacks and the image blending was an interesting project</p>
    </section>
  </main>
</body>
</html>